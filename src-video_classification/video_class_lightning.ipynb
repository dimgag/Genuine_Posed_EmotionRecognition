{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('pytorchvideo')\n",
    "from pytorchvideo.data import LabeledVideoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation process\n",
    "from pytorchvideo.data import LabeledVideoDataset, make_clip_sampler, labeled_video_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "\n",
    "    UniformTemporalSubsample,\n",
    "    Permute\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize\n",
    ")\n",
    "\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_transform = Compose([\n",
    "    ApplyTransformToKey(key='video',\n",
    "        transform = Compose([\n",
    "        UniformTemporalSubsample(20),\n",
    "        Lambda(lambda x:x/255),\n",
    "        NormalizeVideo(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]),\n",
    "        RandomShortSideScale(min_size=248, max_size=256),\n",
    "        CenterCropVideo(224),\n",
    "        RandomHorizontalFlip(p=0.5)\n",
    "    ]),\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeled video dataset\n",
    "train_dataset = labeled_video_dataset('../data_temporal/train_root',\n",
    "                                      clip_sampler=make_clip_sampler('random', 2),\n",
    "                                        transform = video_transform, decode_audio=False)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch['video'].shape, train_batch['label'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset = labeled_video_dataset('../data_temporal/val_root',\n",
    "#                                       clip_sampler=make_clip_sampler('random', 2),\n",
    "#                                         transform = video_transform, decode_audio=False)\n",
    "\n",
    "\n",
    "# val_loader = DataLoader(train_dataset, batch_size=5, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_batch = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_batch['video'].shape, val_batch['label'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, seed_everything, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import classification_report\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_model = torch.hub.load('facebookresearch/pytorchvideo', 'efficient_x3d_xs', pretrained=True)\n",
    "# Other models (https://github.com/facebookresearch/pytorchvideo/tree/main/pytorchvideo/models/hub): \n",
    "# efficient_x3d_s\n",
    "# c2d_r50\n",
    "# i3d_r50\n",
    "# slow_r50\n",
    "# slowfast_r50\n",
    "# slowfast_r101\n",
    "# slowfast_16x8_r101_50_50\n",
    "# csn_r101\n",
    "# r2plus1d_r50\n",
    "# x3d_xs\n",
    "# x3d_s\n",
    "# x3d_m\n",
    "# x3d_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "class VideoModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(VideoModel, self).__init__()\n",
    "        # Model Architecture\n",
    "        self.video_model = torch.hub.load('facebookresearch/pytorchvideo', 'efficient_x3d_xs', pretrained=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(400, 12)\n",
    "        \n",
    "        self.lr = 1e-3\n",
    "        \n",
    "        self.batch_size = 8\n",
    "        self.numworker=4 # If decrease it will be slower computation time\n",
    "        \n",
    "        # Evaluation Metric\n",
    "        self.metric = torchmetrics.Accuracy(task='multiclass',   num_classes=12)\n",
    "        \n",
    "        # Loss Function\n",
    "        # self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.video_model(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(params = self.parameters(), lr = self.lr)\n",
    "        scheduler = CosineAnnealingLR(opt, T_max=10, eta_min=1e-6, last_epoch=-1)\n",
    "        return {'optimizer': opt, 'lr_scheduler': scheduler}\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = labeled_video_dataset('../data_temporal/train_root',\n",
    "                                      clip_sampler=make_clip_sampler('random', 2),\n",
    "                                        transform = video_transform, decode_audio=False)\n",
    "\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.numworker, pin_memory=True)\n",
    "        return loader\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        out = self(video) # or self.forward(video)\n",
    "        loss = self.criterion(out, label)\n",
    "        metric = self.metric(out, label.to(torch.int64))\n",
    "        return {'loss': loss, 'metric': metric.detach()}\n",
    "    \n",
    "    \n",
    "    def on_train_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        self.log('training_loss', loss)\n",
    "        self.log('trainng_metric', metric)\n",
    "    \n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset = labeled_video_dataset('../data_temporal/val_root',\n",
    "                                      clip_sampler=make_clip_sampler('random', 2),\n",
    "                                        transform = video_transform, decode_audio=False)\n",
    "\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.numworker, pin_memory=True)\n",
    "        return loader\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        out = self(video) # or self.forward(video)\n",
    "        loss = self.criterion(out, label)\n",
    "        metric = self.metric(out, label.to(torch.int64))\n",
    "        return {'loss': loss, 'metric': metric.detach()}\n",
    "    \n",
    "    def on_validation_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_metric', metric)\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        dataset = labeled_video_dataset('../data_temporal/val_root',\n",
    "                                      clip_sampler=make_clip_sampler('random', 2),\n",
    "                                        transform = video_transform, decode_audio=False)\n",
    "\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.numworker, pin_memory=True)\n",
    "        return loader\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        out = self(video) # or self.forward(video)\n",
    "        return {'label': label.detach(), 'pred': out.detach()}\n",
    "    \n",
    "    def on_test_epoch_end(self, outputs):\n",
    "        label = torch.cat([x['label'] for x in outputs]).cpu().numpy()\n",
    "        pred = torch.cat([x['pred'] for x in outputs]).cpu().numpy()\n",
    "        pred = np.where(pred>0.5,1,0)\n",
    "        print(classification_report(label, pred))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath = 'checkpoints',\n",
    "                                      filename='file', save_last=True)\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "# 10 (total), 5(no improved), 7(interupted), 7(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VideoModel()\n",
    "seed_everything(0)\n",
    "\n",
    "trainer = Trainer(max_epochs = 1,\n",
    "                  accelerator = 'gpu', devices = -1,\n",
    "                      precision = 16,\n",
    "                  accumulate_grad_batches = 2,\n",
    "                  enable_progress_bar = False,\n",
    "                  num_sanity_val_steps = 0,\n",
    "                  callbacks = [lr_monitor, checkpoint_callback],\n",
    "                  # limit_train_batches = 5, # If you want to limit it \n",
    "                  # limit_val_batches = 1, # If you want to limit it \n",
    "                  \n",
    "                  \n",
    "                 )\n",
    "                  \n",
    "# Next to accelerator I removed this:  , devices = -1, because running on cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation results\n",
    "val_res = trainer.validate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing results\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load the log file in tesnforboard\n",
    "from lightning_fabric.loggers import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir logs/fit #(lightning_logs? Search for this file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue Training from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to continue training from checkpoint:\n",
    "# trainer.fit(model, ckpt_path='checkpoints/last.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
