{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorchvideo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# sys.path.append('pytorchvideo')\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabeledVideoDataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorchvideo'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.append('pytorchvideo')\n",
    "from pytorchvideo.data import LabeledVideoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Augmentation process\n",
    "from pytorchvideo.data import LabeledVideoDataset, make_clip_sampler, labeled_video_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "\n",
    "    UniformTemporalSubsample,\n",
    "    Permute\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize\n",
    ")\n",
    "\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_transform = Compose([\n",
    "    ApplyTransformToKey(key='video',\n",
    "        transform = Compose([\n",
    "        UniformTemporalSubsample(20),\n",
    "        Lambda(lambda x:x/255),\n",
    "        NormalizeVideo(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]),\n",
    "        RandomShortSideScale(min_size=256, max_size=256),\n",
    "        CenterCropVideo(224),\n",
    "        RandomHorizontalFlip(p=0.5)\n",
    "    ]),\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeled video dataset\n",
    "train_dataset = labeled_video_dataset('../data_temporal/train_root',\n",
    "                                      clip_sampler=make_clip_sampler('random', 2),\n",
    "                                        transform = video_transform, decode_audio=False)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch['video'].shape, train_batch['label'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset = labeled_video_dataset('../data_temporal/val_root',\n",
    "#                                       clip_sampler=make_clip_sampler('random', 2),\n",
    "#                                         transform = video_transform, decode_audio=False)\n",
    "\n",
    "\n",
    "# val_loader = DataLoader(train_dataset, batch_size=5, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_batch = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_batch['video'].shape, val_batch['label'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, seed_everything, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import classification_report\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "video_model = torch.hub.load('facebookresearch/pytorchvideo', 'efficient_x3d_xs', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "class OurModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(OurModel, self).__init__()\n",
    "        # Model Architecture\n",
    "        self.video_model = torch.hub.load('facebookresearch/pytorchvideo', 'efficient_x3d_xs', pretrained=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(400, 12)\n",
    "        \n",
    "        self.lr = 1e-3\n",
    "        \n",
    "        self.batch_size = 8\n",
    "        self.numworker=4 # If decrease it will be slower computation time\n",
    "        \n",
    "        # Evaluation Metric\n",
    "        self.metric = torchmetrics.Accuracy(task='multiclass',   num_classes=12)\n",
    "        \n",
    "        # Loss Function\n",
    "        # self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.video_model(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(params = self.parameters(), lr = self.lr)\n",
    "        scheduler = CosineAnnealingLR(opt, T_max=10, eta_min=1e-6, last_epoch=-1)\n",
    "        return {'optimizer': opt, 'lr_scheduler': scheduler}\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = labeled_video_dataset('../data_temporal/train_root',\n",
    "                                      clip_sampler=make_clip_sampler('random', 2),\n",
    "                                        transform = video_transform, decode_audio=False)\n",
    "\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.numworker, pin_memory=True)\n",
    "        return loader\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        out = self(video) # or self.forward(video)\n",
    "        loss = self.criterion(out, label)\n",
    "        metric = self.metric(out, label.to(torch.int64))\n",
    "        return {'loss': loss, 'metric': metric.detach()}\n",
    "    \n",
    "    \n",
    "    def on_train_epoch_end(slef, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        self.log('training_loss', loss)\n",
    "        self.log('trainng_metric', metric)\n",
    "    \n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset = labeled_video_dataset('../data_temporal/val_root',\n",
    "                                      clip_sampler=make_clip_sampler('random', 2),\n",
    "                                        transform = video_transform, decode_audio=False)\n",
    "\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.numworker, pin_memory=True)\n",
    "        return loader\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        out = self(video) # or self.forward(video)\n",
    "        loss = self.criterion(out, label)\n",
    "        metric = self.metric(out, label.to(torch.int64))\n",
    "        return {'loss': loss, 'metric': metric.detach()}\n",
    "    \n",
    "    def on_vaidation_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_metric', metric)\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        dataset = labeled_video_dataset('../data_temporal/val_root',\n",
    "                                      clip_sampler=make_clip_sampler('random', 2),\n",
    "                                        transform = video_transform, decode_audio=False)\n",
    "\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.numworker, pin_memory=True)\n",
    "        return loader\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        out = self(video) # or self.forward(video)\n",
    "        return {'label': label.detach(), 'pred': out.detach()}\n",
    "    \n",
    "    def on_test_epoch_end(self, outputs):\n",
    "        label = torch.cat([x['label'] for x in outputs]).cpu().numpy()\n",
    "        pred = torch.cat([x['pred'] for x in outputs]).cpu().numpy()\n",
    "        pred = np.where(pred>0.5,1,0)\n",
    "        print(classification_report(label, pred))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath = 'checkpoints',\n",
    "                                      filename='file', save_last=True)\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "# 10 (total), 5(no improved), 7(interupted), 7(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "Global seed set to 0\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = OurModel()\n",
    "seed_everything(0)\n",
    "\n",
    "trainer = Trainer(max_epochs = 1,\n",
    "                  accelerator = 'cpu',\n",
    "                      precision = 16,\n",
    "                  accumulate_grad_batches = 2,\n",
    "                  enable_progress_bar = False,\n",
    "                  num_sanity_val_steps = 0,\n",
    "                  callbacks = [lr_monitor, checkpoint_callback],\n",
    "                  # limit_train_batches = 5, # If you want to limit it \n",
    "                  # limit_val_batches = 1, # If you want to limit it \n",
    "                  \n",
    "                  \n",
    "                 )\n",
    "                  \n",
    "# Next to accelerator I removed this:  , devices = -1, because running on cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type               | Params\n",
      "---------------------------------------------------\n",
      "0 | video_model | EfficientX3d       | 3.8 M \n",
      "1 | relu        | ReLU               | 0     \n",
      "2 | linear      | Linear             | 4.8 K \n",
      "3 | metric      | MulticlassAccuracy | 0     \n",
      "4 | criterion   | CrossEntropyLoss   | 0     \n",
      "---------------------------------------------------\n",
      "3.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 M     Total params\n",
      "15.197    Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
